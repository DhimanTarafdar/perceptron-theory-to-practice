{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/perceptron-theory-to-practice/blob/main/DL_Assignment_01_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# DL Assignment 01\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Course Email:**  \n",
        "\n",
        "This is a small assignment that connects topics from Module 1, 2, and 3.  \n",
        "\n",
        "## End of Assignment\n",
        "\n",
        "Before submitting:\n",
        "- Run all cells from top to bottom.  \n",
        "- Check that all answer sections are filled.  \n",
        "- Instruction video অনুযায়ী আমাদের দেয়া Colab ফাইলটি থেকে প্রথম একটি Save copy in drive করে নিবা। এরপর Google colab এর মধ্যে কোডগুলো করবে এবং সেই ফাইলটি ‘Anyone with the link’ & ‘View’ Access দিয়ে ফাইলটির Shareble Link টি সাবমিট করবে।"
      ],
      "metadata": {
        "id": "JcVoA-oxVACn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 01: [ Marks 10 ]\n",
        "\n",
        "What is a perceptron?\n",
        "Explain its three main components and their roles."
      ],
      "metadata": {
        "id": "d18z86IcVO3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write** Answer 01:\n",
        "---\n",
        "### What is a Perceptron?\n",
        "A Perceptron is the most basic unit of Deep Learning. It is a type of artificial neuron that takes some inputs and produces a single output. It works like a simple decision-making model that helps a computer learn basic patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### Three Main Components of a Perceptron:\n",
        "\n",
        "1. **Weights ($w$):** Weights represent the importance or \"strength\" of each input. Not all inputs are equally important. If an input has a higher weight, it has a bigger impact on the final output.\n",
        "\n",
        "2. **Bias ($b$):** Bias is like an extra \"adjustment\" factor. It helps the model make decisions even when all inputs are zero. It allows the activation function to shift up or down to make the model more flexible.\n",
        "\n",
        "3. **Activation Function:** This is the final decision-maker. It takes the total sum of (inputs × weights + bias) and decides whether the neuron should \"fire\" (output 1) or stay \"silent\" (output 0). A common example is the **Step Function**.\n",
        "\n",
        "---\n",
        "\n",
        "### How it works (The Role):\n",
        "The perceptron multiplies each input by its weight, adds them all together with the bias, and then passes the result through the activation function to get the final prediction.\n",
        "\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "uEW6ibGfVZW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 02: [ Marks 10 ]\n",
        "\n",
        "What is a decision boundary in a perceptron?\n",
        "Explain how the equation\n",
        "w₁x₁ + w₂x₂ + b = 0\n",
        "represents a decision boundary geometrically."
      ],
      "metadata": {
        "id": "_tPnXmkMVi8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 02:\n",
        "---\n",
        "### What is a Decision Boundary?\n",
        "A decision boundary is a \"line\" or a \"border\" that separates different classes of data. In a perceptron, it acts as a divider. For example, if we are classifying data into two groups (like \"Yes\" or \"No\"), the decision boundary is the line that tells the model which side a data point belongs to.\n",
        "\n",
        "---\n",
        "\n",
        "### Geometrical Representation of $w_1x_1 + w_2x_2 + b = 0$\n",
        "\n",
        "The equation $w_1x_1 + w_2x_2 + b = 0$ represents a straight line in a 2D plane. Here is how it works geometrically:\n",
        "\n",
        "1. **The Line as a Divider:** Any point $(x_1, x_2)$ that falls exactly on this line results in zero. This line is the boundary.\n",
        "2. **Above the Boundary:** If $w_1x_1 + w_2x_2 + b > 0$, the perceptron predicts one class (e.g., Output = 1).\n",
        "3. **Below the Boundary:** If $w_1x_1 + w_2x_2 + b < 0$, the perceptron predicts the other class (e.g., Output = 0).\n",
        "\n",
        "### Role of Components:\n",
        "* **Weights ($w_1, w_2$):** They determine the **angle or slope** of the line. Changing the weights rotates the boundary.\n",
        "* **Bias ($b$):** It determines the **position** of the line. Changing the bias shifts the line up, down, left, or right, allowing it to move away from the origin (0,0).\n",
        "\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "oTQ9yUrlVk4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 03: [ Marks 15 ]\n",
        "\n",
        "Why is the perceptron called a linear classifier?\n",
        "Explain your answer using the concept of linear separability and logical gate examples such as AND, OR, and XOR."
      ],
      "metadata": {
        "id": "YiUWig4VVnXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 03:\n",
        "---\n",
        "### Why is the Perceptron Called a Linear Classifier?\n",
        "A perceptron is called a linear classifier because it can only separate data using a straight line (in 2D), a flat plane (in 3D), or a hyperplane (in higher dimensions). The decision boundary created by the perceptron is always linear, which means it follows the equation of a straight line: w₁x₁ + w₂x₂ + b = 0.\n",
        "\n",
        "The perceptron cannot create curved or complex decision boundaries. It can only draw straight lines to divide classes.\n",
        "\n",
        "---\n",
        "\n",
        "### Concept of Linear Separability:\n",
        "Linear separability means that two classes of data points can be completely separated by a single straight line (or hyperplane). If we can draw one straight line that perfectly divides all positive examples from all negative examples, then the data is linearly separable.\n",
        "\n",
        "**A perceptron can only solve problems where data is linearly separable.**\n",
        "\n",
        "---\n",
        "\n",
        "### Logical Gate Examples:\n",
        "\n",
        "#### 1. **AND Gate (Linearly Separable):**\n",
        "The AND gate outputs 1 only when both inputs are 1.\n",
        "\n",
        "| x₁ | x₂ | Output |\n",
        "|----|----|----|\n",
        "| 0  | 0  | 0  |\n",
        "| 0  | 1  | 0  |\n",
        "| 1  | 0  | 0  |\n",
        "| 1  | 1  | 1  |\n",
        "\n",
        "If we plot these points, we can draw a straight line that separates the output 1 (point [1,1]) from outputs 0 (points [0,0], [0,1], [1,0]). **A perceptron can solve AND gate.**\n",
        "\n",
        "#### 2. **OR Gate (Linearly Separable):**\n",
        "The OR gate outputs 1 when at least one input is 1.\n",
        "\n",
        "| x₁ | x₂ | Output |\n",
        "|----|----|----|\n",
        "| 0  | 0  | 0  |\n",
        "| 0  | 1  | 1  |\n",
        "| 1  | 0  | 1  |\n",
        "| 1  | 1  | 1  |\n",
        "\n",
        "Here also, we can draw a straight line separating output 0 (point [0,0]) from outputs 1 (points [0,1], [1,0], [1,1]). **A perceptron can solve OR gate.**\n",
        "\n",
        "#### 3. **XOR Gate (NOT Linearly Separable):**\n",
        "The XOR gate outputs 1 when inputs are different.\n",
        "\n",
        "| x₁ | x₂ | Output |\n",
        "|----|----|----|\n",
        "| 0  | 0  | 0  |\n",
        "| 0  | 1  | 1  |\n",
        "| 1  | 0  | 1  |\n",
        "| 1  | 1  | 0  |\n",
        "\n",
        "If we try to plot these points, we cannot draw a single straight line that separates outputs 1 (points [0,1], [1,0]) from outputs 0 (points [0,0], [1,1]). The classes are mixed diagonally. **A perceptron cannot solve XOR gate** because it is not linearly separable.\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kTpWEibjaGmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 04: [ Marks 15 ]\n",
        "\n",
        "Write the perceptron weight update rule and explain it intuitively.\n",
        "Why are the weights updated only when the prediction is incorrect?"
      ],
      "metadata": {
        "id": "Vh-CVlcIZMdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 04:\n",
        "---\n",
        "### The Perceptron Weight Update Rule:\n",
        "The weight update formula is:\n",
        "\n",
        "**w_new = w_old + η × (y_true - y_pred) × x**\n",
        "\n",
        "**b_new = b_old + η × (y_true - y_pred)**\n",
        "\n",
        "Where:\n",
        "- **w_new** = updated weight\n",
        "- **w_old** = current weight\n",
        "- **η** (eta) = learning rate (a small positive number like 0.01 or 0.1)\n",
        "- **y_true** = actual output (correct label)\n",
        "- **y_pred** = predicted output (perceptron's prediction)\n",
        "- **x** = input value\n",
        "\n",
        "---\n",
        "\n",
        "### Intuitive Explanation:\n",
        "The perceptron learns by making mistakes and correcting itself. When it makes a wrong prediction, it adjusts the weights to reduce the error.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **If prediction is correct (y_true = y_pred):**\n",
        "   - The error term (y_true - y_pred) = 0\n",
        "   - So weight update = 0\n",
        "   - Weights don't change\n",
        "\n",
        "2. **If prediction is wrong (y_true ≠ y_pred):**\n",
        "   - The error term (y_true - y_pred) ≠ 0\n",
        "   - Weights get updated to move closer to the correct answer\n",
        "\n",
        "**Example:**\n",
        "- If y_true = 1 but y_pred = 0: error = +1, weights increase\n",
        "- If y_true = 0 but y_pred = 1: error = -1, weights decrease\n",
        "\n",
        "The learning rate (η) controls how big each update step is. Small η means slow learning, large η means fast but unstable learning.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Weights Are Updated Only When Prediction is Incorrect:\n",
        "Because if the perceptron is already making the correct prediction, there is no need to change anything. Updating weights when the answer is already right could make the model worse. We only fix what's broken - this saves computation and prevents unnecessary changes that might hurt performance.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "rfdsvUWfaJJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 05: [ Marks 10 ]\n",
        "\n",
        "Why does a single-layer perceptron fail to solve the XOR problem?\n",
        "What does this limitation tell us about the need for multilayer neural networks?"
      ],
      "metadata": {
        "id": "WIr6YmxaZNli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 05:\n",
        "---\n",
        "### Why Does a Single-Layer Perceptron Fail to Solve XOR?\n",
        "A single-layer perceptron fails to solve the XOR problem because **XOR is not linearly separable**.\n",
        "\n",
        "**The XOR Problem:**\n",
        "\n",
        "| x₁ | x₂ | Output |\n",
        "|----|----|----|\n",
        "| 0  | 0  | 0  |\n",
        "| 0  | 1  | 1  |\n",
        "| 1  | 0  | 1  |\n",
        "| 1  | 1  | 0  |\n",
        "\n",
        "If we plot these points, we see that outputs 0 and 1 are arranged diagonally. We cannot draw a single straight line that separates the two classes. The perceptron can only create linear (straight line) decision boundaries, so it cannot classify XOR correctly.\n",
        "\n",
        "**Simple explanation:**\n",
        "The perceptron tries to find one straight line to divide the data, but XOR needs a curved or multiple boundaries to separate the classes properly.\n",
        "\n",
        "---\n",
        "\n",
        "### What Does This Limitation Tell Us?\n",
        "This limitation shows us that:\n",
        "\n",
        "1. **Real-world problems are often non-linear:** Most practical problems cannot be solved with just a straight line. We need more complex decision boundaries.\n",
        "\n",
        "2. **Need for depth (hidden layers):** To solve non-linear problems like XOR, we need multilayer neural networks (also called deep neural networks). Adding hidden layers between input and output allows the network to learn complex patterns and curved decision boundaries.\n",
        "\n",
        "3. **Combination of simple functions creates complexity:** Multiple perceptrons working together in layers can combine their simple linear decisions to create complex non-linear classifications.\n",
        "\n",
        "**Example:** A 2-layer network can solve XOR by combining multiple linear boundaries to create the needed separation.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "RVJMptexaKjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 06: [ Marks 20 ]\n",
        "\n",
        "What is meant by the “perceptron Learning Rule” or weight adjustment method?\n",
        "Why can adjusting one weight affect the classification of other points?"
      ],
      "metadata": {
        "id": "t7wLPZTPZOnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 06:\n",
        "---\n",
        "### What is the Perceptron Learning Rule?\n",
        "The Perceptron Learning Rule is the method by which a perceptron learns from its mistakes and improves its predictions. It is an iterative algorithm that adjusts weights and bias whenever the perceptron makes a wrong prediction.\n",
        "\n",
        "**The process:**\n",
        "1. Start with random weights and bias\n",
        "2. Make a prediction for each training example\n",
        "3. If prediction is wrong, update weights using the formula:\n",
        "   - **w_new = w_old + η × (y_true - y_pred) × x**\n",
        "4. Repeat until all predictions are correct or a maximum number of iterations is reached\n",
        "\n",
        "This is a **supervised learning** method because we use labeled data (correct answers) to train the perceptron.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Can Adjusting One Weight Affect Other Points?\n",
        "When we adjust a weight to correctly classify one data point, it changes the decision boundary (the separating line). This shift in the decision boundary can affect how other points are classified.\n",
        "\n",
        "**Reason:**\n",
        "All input points share the same weights in the equation **w₁x₁ + w₂x₂ + b = 0**. When we change w₁ or w₂ or b to fix one point, the entire line moves or rotates, which impacts all points in the space.\n",
        "\n",
        "**Simple example:**\n",
        "- Suppose we increase w₁ to correctly classify point A\n",
        "- This tilts the decision boundary\n",
        "- Now point B, which was previously on the correct side, might move to the wrong side\n",
        "- So fixing one point can break the classification of another point\n",
        "\n",
        "This is why the perceptron learning algorithm needs multiple iterations - it keeps adjusting weights until it finds a configuration where all points are correctly classified (if the data is linearly separable).\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "qNmHw4tnaMFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 07: [ Marks 20 ]\n",
        "\n",
        "What are the main limitations of a single-layer perceptron?\n",
        "How does the idea of using more than one layer (MLP) help overcome these limitations at a high level accoriding to you?\n",
        "\n"
      ],
      "metadata": {
        "id": "zZMx7oNaZPqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Answer 07:\n",
        "---\n",
        "### Main Limitations of a Single-Layer Perceptron:\n",
        "\n",
        "1. **Cannot Solve Non-Linear Problems:**\n",
        "   A single-layer perceptron can only create linear (straight line) decision boundaries. It fails on problems that require curved or complex boundaries, like the XOR problem.\n",
        "\n",
        "2. **Limited to Binary Classification:**\n",
        "   Basic perceptron is designed for binary classification (two classes only). It struggles with multi-class classification problems without modifications.\n",
        "\n",
        "3. **Only Linearly Separable Data:**\n",
        "   The perceptron can only work when data is linearly separable. Most real-world data is not linearly separable, so single-layer perceptron has very limited practical use.\n",
        "\n",
        "4. **No Hidden Representations:**\n",
        "   It cannot learn intermediate features or patterns. It directly maps inputs to outputs without creating useful internal representations of the data.\n",
        "\n",
        "5. **Simple Decision Making:**\n",
        "   It can only make very basic decisions and cannot capture complex relationships between input features.\n",
        "\n",
        "---\n",
        "\n",
        "### How Does MLP (Multi-Layer Perceptron) Overcome These Limitations?\n",
        "\n",
        "**MLP = Multi-Layer Perceptron** (a neural network with one or more hidden layers between input and output)\n",
        "\n",
        "1. **Solves Non-Linear Problems:**\n",
        "   Hidden layers allow the network to learn non-linear decision boundaries. Multiple layers can combine simple linear boundaries to create complex curved separations. This is why MLP can solve XOR and other non-linear problems.\n",
        "\n",
        "2. **Learns Feature Representations:**\n",
        "   Hidden layers automatically learn useful features from raw data. Each layer transforms the input into more abstract representations, making classification easier.\n",
        "\n",
        "3. **Handles Complex Patterns:**\n",
        "   With multiple layers, MLP can model very complex relationships and patterns in data that single perceptrons cannot detect.\n",
        "\n",
        "4. **Universal Approximation:**\n",
        "   Mathematically, an MLP with enough hidden neurons can approximate any continuous function. This makes it very powerful for diverse problems.\n",
        "\n",
        "5. **Better for Real-World Applications:**\n",
        "   MLP works on real-world data that is messy and non-linear, making it practical for image recognition, speech processing, and many other tasks.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "cTm1k36iMlDp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ek4VwGsn_3Cs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}